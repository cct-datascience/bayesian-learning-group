{
  "hash": "f6c8feec6d6371a6bced866597ac46b7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Posterior distributionss\"\neditor: source\nfreeze: true\n---\n\n\n## Getting a toy model\n\nSee the model from [last week](components.qmd):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(ggdist)\ntheme_set(theme_minimal())\n\npenguins <- penguins |>\n  filter(!is.na(sex)) |>\n  filter(species == \"Chinstrap\") \n\nggplot(penguins, aes(bill_length_mm, \n                     bill_depth_mm)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](posterior_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n### The model\n\n$\\text{billdepth}_i \\sim N(\\mu, \\sigma)$\n\n$\\mu_i = \\alpha + \\beta(\\text{billlength}_i - \\text{mean}(\\text{billlength}))$\n\n$\\alpha \\sim N(20, 6)$\n\n$\\beta \\sim N(0, 5)$\n\n$\\sigma \\sim \\text{Uniform}(0, 20)$\n\n### The code\n\n### Fitting the model (with data)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepth_length_brm <- brm(\n  family = gaussian,\n  data = penguins,\n  formula = bill_depth_mm ~ bill_length_mm,\n  prior = c(\n      prior(normal(20, 6), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(uniform(0, 20), class = sigma, ub = 20)),\n  iter = 1000\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(depth_length_brm)\n```\n\n::: {.cell-output-display}\n![](posterior_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## The posterior distribution\n\n[This bookdown book](https://bookdown.org/kevin_davisross/bayesian-reasoning-and-methods/) is an excellent resource and is the background of these materials.\n\nThe posterior distribution gives the likely distributions of the *parameters* of a model taking into account the *prior*, the *data*, and the model *structure* we defined earlier (in this case a linear model of bill depth as a function of bill length).\n\n### MCMC checks\n\nWe characterize the posterior by sampling from the posterior using Markov Chain Monte Carlo.\nMCMC uses \"chains\" to probabilistically explore the landscape of possible parameter values.\nMCMC *should* give us sets of simulated parameter values where more likely values are represented more frequently (proportional to how likely they are).\nBut, after a model runs it is a good idea to check diagnostics on the MCMC procedure to make sure it's gone smoothly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(depth_length_brm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_depth_mm ~ bill_length_mm \n   Data: penguins (Number of observations: 68) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          7.57      1.60     4.29    10.62 1.00     1828     1302\nbill_length_mm     0.22      0.03     0.16     0.29 1.00     1803     1301\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.88      0.08     0.75     1.05 1.00     2180     1472\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nDiagnostics:\n\n-   Trace plots should look like \"hairy caterpillars\"\n-   Convergence: Rhat should approach 1\n-   ESS should be high ([One page I found](https://easystats.github.io/bayestestR/reference/effective_sample.html) offers 1000 as a rule of thumb; `brms`/`stan` will print a warning about low ESS).\n\n### Posterior predictive checks\n\n[Here is an excellent blog post on posterior predictive distributions and marginal effects](https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/#expected-values-of-the-posterior-predictive-distribution).\n\nThe posterior distribution gives us distributions for the *parameters*.\nWe want to use these to generate distributions for the *data* that we would get if we give the original predictor variables to the model.\nIf the model says that the data we observe is highly unlikely - i.e. the posterior predictions are far off from the observed data - we should be suspicious.\n\nThe `predicted_draws` function from the `tidybayes` package generates posterior predictive distributions.\nIf you provide the original input data as `newdata`, it will simulate values for the response variable based on the original predictor variables plus the parameters from draws from the posterior.\n\nIn this plot, the real data are in blue:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_draws <- tidybayes::predicted_draws(\n  depth_length_brm, \n  newdata = penguins,\n  ndraws = 100\n  )\n\nggplot(posterior_draws, aes(bill_length_mm, .prediction)) + \n  geom_point(alpha = .3) +\n  geom_point(aes(y = bill_depth_mm), color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](posterior_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "posterior_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}