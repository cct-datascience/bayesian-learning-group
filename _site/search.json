[
  {
    "objectID": "spring2024.html",
    "href": "spring2024.html",
    "title": "Spring 2024 Schedule",
    "section": "",
    "text": "See the group HackMD for notes from the Spring series."
  },
  {
    "objectID": "spring2024.html#notes",
    "href": "spring2024.html#notes",
    "title": "Spring 2024 Schedule",
    "section": "",
    "text": "See the group HackMD for notes from the Spring series."
  },
  {
    "objectID": "spring2024.html#schedule",
    "href": "spring2024.html#schedule",
    "title": "Spring 2024 Schedule",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nDate\nAsynchronous content\n(we suggest reading OR watching)\nIn-session\nLeader\n\n\n\n\n1/19/2024\n\nCh. 1 The Golem of Prague\nLecture 1: Science before statistics\n\n\nIntroductions\nGroup setup and format\nSoftware setup\nDiscuss the drawing of owls\n\nJessica, Renata\n\n\n2/2/2024\nTime change: Meet from 11-1 instead of 12-2!\n\nCh. 2 Small Worlds and Large Worlds\nCh. 3 Sampling the Imaginary\nLecture 2: The Garden of Forking Data\n\n\nCode up Garden of Forking Data example\nChallenge problems here!\nExercises in brms book sections 2 and 3\n\nJessica, Renata\n\n\n2/16/2024\n\nCh. 4 Linear Models\nLecture 3: Geocentric Models (skip to 19:00 if you are familiar with Gaussian distributions)\nLecture 4: Lines and Curves\n\n\nChallenge problems from McElreath’s course here\nExercises in brms book\nCheck pacing and set rest of schedule\n\n\n\n\n3/1/2024\n\nN/A\n\n\nDemo (Jessica)\n\n\n\n\n3/15/2024\n\nContinue Ch 4.\n\n\nCh. 4\nDemo (Val)\n\n\n\n\n3/29/2024\n\nCh. 5 Many Variables and Spurious Waffles\n\n\n\nCh. 6 The Haunted DAG and Causal Terror\n\n\nCh 6 Problems TBD\n\n\n\n\n4/12/2024\n\nCh. 7 Ulysses’ Compass\nCh. 8 Conditional Manatees\n\n\nCh. 7-8 Problems TBD\n\n\n\n\n4/26/2024\n\nCh. 9 MCMC\n\n\nCh. 9 Problems\nWrap-up and Planning"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Summer 2024 HackMD\nSpring 2024 HackMD\n\n\n\nSpring schedule"
  },
  {
    "objectID": "resources.html#hackmd-notes",
    "href": "resources.html#hackmd-notes",
    "title": "Resources",
    "section": "",
    "text": "Summer 2024 HackMD\nSpring 2024 HackMD"
  },
  {
    "objectID": "resources.html#spring-schedule",
    "href": "resources.html#spring-schedule",
    "title": "Resources",
    "section": "",
    "text": "Spring schedule"
  },
  {
    "objectID": "resources.html#the-book",
    "href": "resources.html#the-book",
    "title": "Resources",
    "section": "The book",
    "text": "The book\nThe 2020 edition is available as an eBook through the UA library. Permalink here.\nThe 2018 (permalink here) edition is also available. The chapters differ slightly. We will be using the 2020 edition."
  },
  {
    "objectID": "resources.html#video-lectures",
    "href": "resources.html#video-lectures",
    "title": "Resources",
    "section": "Video lectures",
    "text": "Video lectures\nHere is the playlist of McElreath’s 2023 lectures."
  },
  {
    "objectID": "resources.html#homework-exercises",
    "href": "resources.html#homework-exercises",
    "title": "Resources",
    "section": "Homework exercises",
    "text": "Homework exercises\nMcElreath has posted problem sets corresponding to the lectures above."
  },
  {
    "objectID": "resources.html#r-implementations",
    "href": "resources.html#r-implementations",
    "title": "Resources",
    "section": "R implementations",
    "text": "R implementations\n\nrethinking\nMcElreath’s rethinking package.\nTo install rethinking, you will first need to install cmdstanr. Follow the instructions here. Note that if you do not have CmdStan and a C++ toolchain installed, you’ll need to install those as well. There are instructions here. This can take a while!\n\n\nbrms\nThis bookdown book works through the examples and problem questions in McElreath’s book using brms.\nHere’s a link to the brms package.\n\n\nrstan\nHere is an implementation in R and stan."
  },
  {
    "objectID": "components.html",
    "href": "components.html",
    "title": "Components of a model",
    "section": "",
    "text": "Based on the book section 4.2. A language for describing models."
  },
  {
    "objectID": "components.html#variables",
    "href": "components.html#variables",
    "title": "Components of a model",
    "section": "Variables",
    "text": "Variables\n\nData\nObservable things.\n\n\nParameters\nNot observable or known."
  },
  {
    "objectID": "components.html#joint-generative-model",
    "href": "components.html#joint-generative-model",
    "title": "Components of a model",
    "section": "“Joint generative model”",
    "text": "“Joint generative model”\nWe define each variable either in terms of a probability distribution, or in terms of its relationship to the other variables.\nTaken together, this system of definitions is what McElreath calls a “joint generative model”."
  },
  {
    "objectID": "components.html#penguins-example",
    "href": "components.html#penguins-example",
    "title": "Components of a model",
    "section": "Penguins example",
    "text": "Penguins example\nLet’s model bill_length for Chinstrap penguins. First, we won’t use any predictor variables - just looking for the mean.\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(ggdist)\ntheme_set(theme_minimal())\n\npenguins &lt;- penguins |&gt;\n  filter(!is.na(sex)) |&gt;\n  filter(species == \"Chinstrap\") \n\nggplot(penguins, aes(bill_depth_mm)) +\n  geom_dotsinterval()\n\n\n\n\n\n\n\n\n\nDefining the model\n\\(\\text{billdepth}_i \\sim N(\\mu, \\sigma)\\)\nThis just says that bill depth is normally distributed with some mean and some standard deviation. We can observe bill depth (data), but we don’t know the mean or standard deviation (parameters).\nFor the parameters, we need priors.\n\\(\\mu \\sim N(50, 15)\\)\nThis is to say that \\(\\mu\\) is some number drawn from a normal distribution centered on 50:\n\ndata.frame(mu = rnorm(1000, mean = 20, sd = 6)) |&gt;\n  ggplot(aes(x = mu)) +\n  geom_dotsinterval() +\n  ggtitle(\"Simulations from prior for mu\")\n\n\n\n\n\n\n\n\n$(0, 20)$\nAnd here we’re saying that we figure \\(sigma\\) is potentially uniformly distributed ranging from 0-20. Standard deviations have to be positive, and otherwise this is a very broad range.\n\ndata.frame(sigma = runif(1000, 0, 20)) |&gt;\n  ggplot(aes(x = sigma)) +\n  geom_dotsinterval() +\n  ggtitle(\"Simulations from prior for sigma\")\n\n\n\n\n\n\n\n\n\n\nExploring the generative model\n\nsample_mu &lt;- rnorm(1000, 20, 6)\nsample_sigmas &lt;- runif(1000, 0, 20)\n\ndata.frame(simulated_bill_depths = \n             rnorm(1000, \n                   sample_mu,\n                   sample_sigmas)) |&gt;\n  ggplot(aes(simulated_bill_depths)) +\n  geom_dotsinterval() +\n  ggtitle(\"Simlated bill depths from priors\")\n\n\n\n\n\n\n\n\nThis is our simulation of expected bill depths before explicitly taking into account the data (although we did look at the density plot before specifying the priors).\n\n\nTaking into account the data\nSee here for translations of rethinking code to brms.\n\nlibrary(brms)\n\nbill_depth_brm &lt;-\n  brm(\n    family = gaussian,\n    bill_depth_mm ~ 1,\n    data = penguins,\n    prior = c(\n      prior(normal(20, 6), class = Intercept),\n      prior(uniform(0, 20), class = sigma, ub = 20)\n    ),\n    iter = 1000\n  )\n\n\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following objects are masked from 'package:ggdist':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\n\nplot(bill_depth_brm)\n\n\n\n\n\n\n\n\n\nsummary(bill_depth_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_depth_mm ~ 1 \n   Data: penguins (Number of observations: 68) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    18.42      0.14    18.13    18.70 1.00     1390     1244\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.16      0.10     0.98     1.36 1.00     1501     1307\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nExploring the posterior\n\nposterior &lt;- as_draws_df(bill_depth_brm)\n\nhead(posterior)\n\n# A draws_df: 6 iterations, 1 chains, and 5 variables\n  b_Intercept sigma Intercept lprior lp__\n1          19   1.2        19   -5.7 -111\n2          18   1.1        18   -5.7 -111\n3          18   1.1        18   -5.7 -111\n4          18   1.1        18   -5.7 -111\n5          18   1.1        18   -5.7 -111\n6          18   1.0        18   -5.7 -111\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\n\nggplot(posterior, aes(Intercept)) +\n  stat_dotsinterval()\n\n\n\n\n\n\n\nggplot(posterior, aes(sigma)) +\n  stat_dotsinterval()\n\n\n\n\n\n\n\n\n\nposterior_summary(bill_depth_brm)\n\n               Estimate   Est.Error         Q2.5       Q97.5\nb_Intercept   18.421378 0.140472861   18.1294656   18.696755\nsigma          1.155849 0.098822889    0.9816352    1.359573\nIntercept     18.421378 0.140472861   18.1294656   18.696755\nlprior        -5.741316 0.006166867   -5.7550261   -5.730020\nlp__        -111.280959 1.026213063 -114.1671862 -110.321475"
  },
  {
    "objectID": "components.html#penguins-with-a-predictor",
    "href": "components.html#penguins-with-a-predictor",
    "title": "Components of a model",
    "section": "Penguins with a predictor",
    "text": "Penguins with a predictor\nOK, now let’s model bill_depth as a function of bill_length.\n\nggplot(penguins, aes(bill_length_mm, \n                     bill_depth_mm)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nSpecifying the model\n\\(\\text{billdepth}_i \\sim N(\\mu, \\sigma)\\)\nHere, again, we say that bill depth is normally distributed with some mean and standard deviation.\n\\(\\mu_i = \\alpha + \\beta(\\text{billlength}_i - \\text{mean}(\\text{billlength}))\\)\nThis addition makes it into a linear model. Instead of estimating \\(\\mu\\) from the data, we say that the mean of bill depth varies with bill length as linear function with an intercept \\(\\alpha\\) and a slope \\(\\beta\\).\n\\(\\alpha\\) and \\(\\beta\\) are now additional parameters that we will estimate and therefore need to set priors for.\n\\(\\alpha \\sim N(20, 6)\\)\n\\(\\beta \\sim N(0, 5)\\)\n\\(\\sigma \\sim \\text{Uniform}(0, 20)\\)\n\n\nExploring the generative model\n\nprior_draw &lt;- data.frame(\n  bill_length = seq(min(penguins$bill_length_mm), max(penguins$bill_length_mm), length.out = 100),\n  intercept = rnorm(1, 20, 6),\n  beta = rnorm(1, 0, 5),\n  sigma = runif(1, 0, 20)\n) |&gt;\n  mutate(mu = intercept + beta * (bill_length - mean(bill_length))) |&gt;\n  rowwise() |&gt;\n  mutate(sim_depth = rnorm(1, mu, sigma)) \n\n\nggplot(prior_draw, aes(bill_length, mu)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = mu - sigma,\n                  ymax = mu + sigma),\n              alpha = .3) +\n  geom_point(aes(y = sim_depth)) +\n  ggtitle(\"Simulated bill depths\",\n          subtitle = \"Based on a single draw from the priors\")\n\n\n\n\n\n\n\n\n\n\nFitting the model (with data)\n\ndepth_length_brm &lt;- brm(\n  family = gaussian,\n  data = penguins,\n  formula = bill_depth_mm ~ bill_length_mm,\n  prior = c(\n      prior(normal(20, 6), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(uniform(0, 20), class = sigma, ub = 20)),\n  iter = 1000\n)\n\n\nplot(depth_length_brm)\n\n\n\n\n\n\n\n\n\nsummary(depth_length_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_depth_mm ~ bill_length_mm \n   Data: penguins (Number of observations: 68) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          7.57      1.60     4.29    10.62 1.00     1828     1302\nbill_length_mm     0.22      0.03     0.16     0.29 1.00     1803     1301\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.88      0.08     0.75     1.05 1.00     2180     1472\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nExploring the posterior\n\nlm_posterior &lt;- as_draws_df(depth_length_brm)\n\nhead(lm_posterior)\n\n# A draws_df: 6 iterations, 1 chains, and 6 variables\n  b_Intercept b_bill_length_mm sigma Intercept lprior lp__\n1         7.4             0.23  0.96        19   -8.3  -97\n2         7.0             0.24  0.94        19   -8.3  -96\n3         7.8             0.22  0.89        19   -8.3  -96\n4         7.6             0.23  0.79        19   -8.3  -96\n5         7.4             0.22  0.93        18   -8.3  -96\n6         8.9             0.20  0.92        18   -8.3  -95\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\n\nggplot(lm_posterior, aes(b_Intercept)) +\n  stat_dotsinterval()\n\n\n\n\n\n\n\nggplot(lm_posterior, aes(b_bill_length_mm)) +\n  stat_dotsinterval()\n\n\n\n\n\n\n\nggplot(lm_posterior, aes(sigma)) +\n  stat_dotsinterval()\n\n\n\n\n\n\n\n\n\nposterior_summary(depth_length_brm)\n\n                    Estimate   Est.Error        Q2.5       Q97.5\nb_Intercept        7.5678375 1.597685969   4.2885750  10.6186846\nb_bill_length_mm   0.2222723 0.032584203   0.1603925   0.2888031\nsigma              0.8829648 0.077046605   0.7505312   1.0523065\nIntercept         18.4222455 0.108770766  18.2085474  18.6326642\nlprior            -8.2705540 0.004796696  -8.2807804  -8.2617956\nlp__             -95.6697446 1.293997087 -99.0571858 -94.2660701"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the website!"
  },
  {
    "objectID": "index.html#about-the-learning-group",
    "href": "index.html#about-the-learning-group",
    "title": "Home",
    "section": "About the learning group",
    "text": "About the learning group\nThe Bayesian Learning Group is an informal learning community organized by the University of Arizona CCT Data Science team. The initial series of the BLG was offered in Spring 2024 by Jessica Guo and Renata Diaz. We are now continuing into Summer 2024!"
  },
  {
    "objectID": "index.html#group-objectives",
    "href": "index.html#group-objectives",
    "title": "Home",
    "section": "Group objectives",
    "text": "Group objectives\n\nBecome experts in Bayesian statistics! Or, at least, absorb the philosophy and conceptual underpinnings of a Bayesian approach, and learn to build and interpret Bayesian models using modern software packages (mostly brms).\nSupport each other in this quest. Collectively we have a diversity of research interests, backgrounds in statistics and programming, and objectives for the semester. We will share our knowledge and our curiosity, and we will welcome questions, experiments, and intellectual risk-taking. We will collectively commit to maintaining a safe and welcoming environment for all (see the Code of Conduct, below)."
  },
  {
    "objectID": "index.html#learning-group-structure",
    "href": "index.html#learning-group-structure",
    "title": "Home",
    "section": "Learning group structure",
    "text": "Learning group structure\n\nThe learning group is somewhat of an experiment and evolves over time.\nFor summer 2024, sessions will focus on working through examples from Richard McElreath’s Statistical Rethinking using brms. You are encouraged to read the corresponding chapters and/or watch the accompanying video lectures in between sessions, but we will try to provide enough background context to make working through the examples a valuable exercise on its own.\nWe will be starting from Chapter 9 (MCMC). Some background in statistics is helpful, but interest is more important! You do not need to have participated in the spring series, or to have attended previous sessions, to join any session throughout the semester.\nWe also welcome model “show-and-tell” (where volunteers talk us through all of the nerdy model details you usually don’t get to talk about in your papers and talks!) and guest speakers throughout the semester."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Home",
    "section": "Code of Conduct",
    "text": "Code of Conduct\n\nOur group’s mission is to enable scientists. This means treating people with respect and responding in a polite and helpful way.\nOur group is committed to ensuring a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion.\nExamples of unacceptable behavior by members, collaborators, and contributors include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\n\nRead our full code of conduct and please report any violations or concerns to the course instructors or to Kristina Riemer (kristinariemer@arizona.edu)."
  },
  {
    "objectID": "index.html#questions",
    "href": "index.html#questions",
    "title": "Home",
    "section": "Questions?",
    "text": "Questions?\nContact Renata Diaz at renatadiaz@arizona.edu."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Date\nAsynchronous content\nIn-session\n\n\n\n\n5/24/24\n\nCh. 9 MCMC\nMCMC lecture\n\n\nIntroductions\nGroup setup and format\nSoftware setup\nMCMC examples\n\n\n\n6/7/24\n\nScience Before Statistics\nThe Garden of Forking Data\n\nand/or\n\nGlance over Chapters 1-2.\n\n\nDiscuss the parts of a model: model, data, priors, posterior\n\n\n\n6/21/24\n\nGeocentric models\nLines and Curves\n\nand/or\n\nGlance over Ch. 2-3\n\n\nPractice putting together a model and sampling the posterior\n\n\n\n7/5/24\n\nMCMC\nand/or\nCh. 9 MCMC\n\n\nWorked examples with MCMC\n\n\n\n7/19/24\n\nModeling Events\nCounts and Hidden Confounds\nand/or\nCh. 10\n\n\nGeneralized linear models I\n\n\n\n8/2/24\n\nModeling Events\nCounts and Hidden Confounds\nand/or\nCh. 11\n\n\nGeneralized linear models II"
  },
  {
    "objectID": "posterior.html",
    "href": "posterior.html",
    "title": "Posterior distributions",
    "section": "",
    "text": "See the model from last week:\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(ggdist)\ntheme_set(theme_minimal())\n\npenguins &lt;- penguins |&gt;\n  filter(!is.na(sex)) |&gt;\n  filter(species == \"Chinstrap\") \n\nggplot(penguins, aes(bill_length_mm, \n                     bill_depth_mm)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\\(\\text{billdepth}_i \\sim N(\\mu, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta(\\text{billlength}_i - \\text{mean}(\\text{billlength}))\\)\n\\(\\alpha \\sim N(20, 6)\\)\n\\(\\beta \\sim N(0, 5)\\)\n\\(\\sigma \\sim \\text{Uniform}(0, 20)\\)\n\n\n\n\n\n\n\ndepth_length_brm &lt;- brm(\n  family = gaussian,\n  data = penguins,\n  formula = bill_depth_mm ~ bill_length_mm,\n  prior = c(\n      prior(normal(20, 6), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(uniform(0, 20), class = sigma, ub = 20)),\n  iter = 1000\n)\n\n\nplot(depth_length_brm)"
  },
  {
    "objectID": "posterior.html#getting-a-toy-model",
    "href": "posterior.html#getting-a-toy-model",
    "title": "Posterior distributions",
    "section": "",
    "text": "See the model from last week:\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(ggdist)\ntheme_set(theme_minimal())\n\npenguins &lt;- penguins |&gt;\n  filter(!is.na(sex)) |&gt;\n  filter(species == \"Chinstrap\") \n\nggplot(penguins, aes(bill_length_mm, \n                     bill_depth_mm)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\\(\\text{billdepth}_i \\sim N(\\mu, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta(\\text{billlength}_i - \\text{mean}(\\text{billlength}))\\)\n\\(\\alpha \\sim N(20, 6)\\)\n\\(\\beta \\sim N(0, 5)\\)\n\\(\\sigma \\sim \\text{Uniform}(0, 20)\\)\n\n\n\n\n\n\n\ndepth_length_brm &lt;- brm(\n  family = gaussian,\n  data = penguins,\n  formula = bill_depth_mm ~ bill_length_mm,\n  prior = c(\n      prior(normal(20, 6), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(uniform(0, 20), class = sigma, ub = 20)),\n  iter = 1000\n)\n\n\nplot(depth_length_brm)"
  },
  {
    "objectID": "posterior.html#the-posterior-distribution",
    "href": "posterior.html#the-posterior-distribution",
    "title": "Posterior distributions",
    "section": "The posterior distribution",
    "text": "The posterior distribution\nThis bookdown book is an excellent resource and is the background of these materials.\nThe posterior distribution gives the likely distributions of the parameters of a model taking into account the prior, the data, and the model structure we defined earlier (in this case a linear model of bill depth as a function of bill length).\n\nMCMC checks\nWe characterize the posterior by sampling from the posterior using Markov Chain Monte Carlo. MCMC uses “chains” to probabilistically explore the landscape of possible parameter values. MCMC should give us sets of simulated parameter values where more likely values are represented more frequently (proportional to how likely they are). But, after a model runs it is a good idea to check diagnostics on the MCMC procedure to make sure it’s gone smoothly.\n\nsummary(depth_length_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_depth_mm ~ bill_length_mm \n   Data: penguins (Number of observations: 68) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          7.57      1.60     4.29    10.62 1.00     1828     1302\nbill_length_mm     0.22      0.03     0.16     0.29 1.00     1803     1301\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.88      0.08     0.75     1.05 1.00     2180     1472\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nDiagnostics:\n\nTrace plots should look like “hairy caterpillars”\nConvergence: Rhat should approach 1\nESS should be high (One page I found offers 1000 as a rule of thumb; brms/stan will print a warning about low ESS).\n\n\n\nPosterior predictive checks\nHere is an excellent blog post on posterior predictive distributions and marginal effects.\nThe posterior distribution gives us distributions for the parameters. We want to use these to generate distributions for the data that we would get if we give the original predictor variables to the model. If the model says that the data we observe is highly unlikely - i.e. the posterior predictions are far off from the observed data - we should be suspicious.\nThe predicted_draws function from the tidybayes package generates posterior predictive distributions. If you provide the original input data as newdata, it will simulate values for the response variable based on the original predictor variables plus the parameters from draws from the posterior.\nIn this plot, the real data are in blue:\n\nposterior_draws &lt;- tidybayes::predicted_draws(\n  depth_length_brm, \n  newdata = penguins,\n  ndraws = 100\n  )\n\nggplot(posterior_draws, aes(bill_length_mm, .prediction)) + \n  geom_point(alpha = .3) +\n  geom_point(aes(y = bill_depth_mm), color = \"blue\")"
  },
  {
    "objectID": "categorical.html",
    "href": "categorical.html",
    "title": "Categorical models and model comparison",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following objects are masked from 'package:ggdist':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nlibrary(tidybayes)\n\n\nAttaching package: 'tidybayes'\n\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\nlibrary(tidyr)\ntheme_set(theme_minimal())\n\npenguins &lt;- penguins |&gt;\n  filter(!is.na(sex),\n         species == \"Chinstrap\") \n\nggplot(penguins, aes(bill_length_mm, \n                     bill_depth_mm, \n                     color = sex)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nSay we want to model bill_length as a function of both bill_depth and sex.\nThere are different approaches to adding a categorial predictor to a model, sometimes called “indicator” (or “dummy”) variables, or “index” variables. McElreath comes down strongly in favor of using “index” variables. For more discussion, see here and links therein.\n\n\n\n\n\n\n\nFirst we can fit a model that will just fit different intercepts for the different species:\n\ndepth_length_sex_brm &lt;- brm(\n  family = gaussian,\n  data = penguins,\n  formula = bill_length_mm ~ 0 + bill_depth_mm + sex,\n  prior = c(\n      prior(normal(0, 5), class = b),\n      prior(uniform(0, 20), class = sigma, ub = 20)),\n  iter = 1000\n)\n\n\n\n\nModel diagnostics:\n\nsummary(depth_length_sex_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 0 + bill_depth_mm + sex \n   Data: penguins (Number of observations: 68) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nbill_depth_mm     2.35      0.17     2.03     2.70 1.00      401      574\nsexfemale         5.19      2.91    -0.90    10.76 1.00      401      535\nsexmale           5.79      3.20    -0.83    11.96 1.00      410      575\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.63      0.25     2.20     3.25 1.01      502      502\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(depth_length_sex_brm)\n\n\n\n\n\n\n\n\nComparing the intercepts for sex:\n\nas_draws_df(depth_length_sex_brm) %&gt;% \n  mutate(diff_fm = b_sexfemale - b_sexmale) %&gt;% \n  pivot_longer(cols = c(b_sexfemale:sigma, diff_fm)) %&gt;% \n  group_by(name) %&gt;% \n  mean_qi(value, .width = .89)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n# A tibble: 4 × 7\n  name         value .lower .upper .width .point .interval\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 b_sexfemale  5.19   0.644  9.82    0.89 mean   qi       \n2 b_sexmale    5.79   0.651 10.9     0.89 mean   qi       \n3 diff_fm     -0.598 -1.72   0.511   0.89 mean   qi       \n4 sigma        2.63   2.27   3.06    0.89 mean   qi       \n\n\n\n\n\nSee chapter 7 and the bookdown book here for more on model comparison criteria.\n\ndepth_length_sex_brm &lt;- depth_length_sex_brm |&gt; add_criterion(criterion = c(\"waic\", \"loo\"))\n\nWarning: \n1 (1.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\ndepth_length_sex_brm$criteria\n\n$waic\n\nComputed from 2000 by 68 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -164.2 10.9\np_waic         5.0  3.0\nwaic         328.4 21.9\n\n1 (1.5%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n$loo\n\nComputed from 2000 by 68 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -164.2 10.9\np_loo         5.0  3.0\nlooic       328.4 21.8\n------\nMCSE of elpd_loo is 0.2.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.2, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nCompare this to our old model, with no term for sex:\n\ndepth_length_brm &lt;- depth_length_brm |&gt; add_criterion(criterion = c(\"waic\", \"loo\"))\n\nWarning: \n1 (1.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\ndepth_length_brm$criteria\n\n$waic\n\nComputed from 2000 by 68 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic    -89.2  6.8\np_waic         3.6  1.7\nwaic         178.4 13.5\n\n1 (1.5%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n$loo\n\nComputed from 2000 by 68 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo    -89.2  6.8\np_loo         3.6  1.7\nlooic       178.4 13.5\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details."
  },
  {
    "objectID": "categorical.html#adding-categorical-predictors",
    "href": "categorical.html#adding-categorical-predictors",
    "title": "Categorical models and model comparison",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following objects are masked from 'package:ggdist':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nlibrary(tidybayes)\n\n\nAttaching package: 'tidybayes'\n\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\nlibrary(tidyr)\ntheme_set(theme_minimal())\n\npenguins &lt;- penguins |&gt;\n  filter(!is.na(sex),\n         species == \"Chinstrap\") \n\nggplot(penguins, aes(bill_length_mm, \n                     bill_depth_mm, \n                     color = sex)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nSay we want to model bill_length as a function of both bill_depth and sex.\nThere are different approaches to adding a categorial predictor to a model, sometimes called “indicator” (or “dummy”) variables, or “index” variables. McElreath comes down strongly in favor of using “index” variables. For more discussion, see here and links therein.\n\n\n\n\n\n\n\nFirst we can fit a model that will just fit different intercepts for the different species:\n\ndepth_length_sex_brm &lt;- brm(\n  family = gaussian,\n  data = penguins,\n  formula = bill_length_mm ~ 0 + bill_depth_mm + sex,\n  prior = c(\n      prior(normal(0, 5), class = b),\n      prior(uniform(0, 20), class = sigma, ub = 20)),\n  iter = 1000\n)\n\n\n\n\nModel diagnostics:\n\nsummary(depth_length_sex_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 0 + bill_depth_mm + sex \n   Data: penguins (Number of observations: 68) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nbill_depth_mm     2.35      0.17     2.03     2.70 1.00      401      574\nsexfemale         5.19      2.91    -0.90    10.76 1.00      401      535\nsexmale           5.79      3.20    -0.83    11.96 1.00      410      575\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.63      0.25     2.20     3.25 1.01      502      502\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(depth_length_sex_brm)\n\n\n\n\n\n\n\n\nComparing the intercepts for sex:\n\nas_draws_df(depth_length_sex_brm) %&gt;% \n  mutate(diff_fm = b_sexfemale - b_sexmale) %&gt;% \n  pivot_longer(cols = c(b_sexfemale:sigma, diff_fm)) %&gt;% \n  group_by(name) %&gt;% \n  mean_qi(value, .width = .89)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n# A tibble: 4 × 7\n  name         value .lower .upper .width .point .interval\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 b_sexfemale  5.19   0.644  9.82    0.89 mean   qi       \n2 b_sexmale    5.79   0.651 10.9     0.89 mean   qi       \n3 diff_fm     -0.598 -1.72   0.511   0.89 mean   qi       \n4 sigma        2.63   2.27   3.06    0.89 mean   qi       \n\n\n\n\n\nSee chapter 7 and the bookdown book here for more on model comparison criteria.\n\ndepth_length_sex_brm &lt;- depth_length_sex_brm |&gt; add_criterion(criterion = c(\"waic\", \"loo\"))\n\nWarning: \n1 (1.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\ndepth_length_sex_brm$criteria\n\n$waic\n\nComputed from 2000 by 68 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -164.2 10.9\np_waic         5.0  3.0\nwaic         328.4 21.9\n\n1 (1.5%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n$loo\n\nComputed from 2000 by 68 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -164.2 10.9\np_loo         5.0  3.0\nlooic       328.4 21.8\n------\nMCSE of elpd_loo is 0.2.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.2, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nCompare this to our old model, with no term for sex:\n\ndepth_length_brm &lt;- depth_length_brm |&gt; add_criterion(criterion = c(\"waic\", \"loo\"))\n\nWarning: \n1 (1.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\ndepth_length_brm$criteria\n\n$waic\n\nComputed from 2000 by 68 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic    -89.2  6.8\np_waic         3.6  1.7\nwaic         178.4 13.5\n\n1 (1.5%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n$loo\n\nComputed from 2000 by 68 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo    -89.2  6.8\np_loo         3.6  1.7\nlooic       178.4 13.5\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details."
  }
]